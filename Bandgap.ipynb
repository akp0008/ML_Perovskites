{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8be83ed-589e-4b63-82b1-79b3938acafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153c748d-6bcf-4481-bebf-f404b078e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('dataset.csv')\n",
    "mask=df1['F']==3\n",
    "df2=df1[~mask].copy()\n",
    "ndf = df2.drop(['HOIP entry ID', 'Label',\n",
    "       'Dielectric constant, total','Dielectric constant, electronic', 'Dielectric constant, ionic', 'Volume of the unit cell (A^3)',\n",
    "        'Refractive index', 'A SITE DFE', 'B SITE DFE', 'X SITE DFE','F'],axis=1)\n",
    "X = ndf.drop(['Bandgap, GGA (eV)'],axis=1)\n",
    "y = ndf['Bandgap, GGA (eV)']\n",
    "X.rename(columns={'Atomization energy (eV/atom)':'E$_{atomization}$', 'Relative energy1 (eV/atom)':'E$_{relative1}$',\n",
    "                   'Relative energy2 (eV/atom)':'E$_{relative2}$','Density (g/cm^3)':'Density','rA(Ang)':'r$_A$','rB(Ang)':'r$_B$','rX(Ang)':'r$_X$'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7382f10-ff25-4161-92af-20d6ab8aabbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset (example data split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Discretized and Dynamic Hyperparameter Search\n",
    "def objective(trial):\n",
    "    # Discretize learning rate and other key hyperparameters\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3])\n",
    "    depth = trial.suggest_int('depth', 4, 10, step=1)  # Use steps for more structured exploration\n",
    "    iterations = trial.suggest_int('iterations', 500, 2000, step=100)\n",
    "    l2_leaf_reg = trial.suggest_categorical('l2_leaf_reg', [1e-3, 0.01, 0.1, 1, 5, 10])\n",
    "    subsample = trial.suggest_float('subsample', 0.7, 1.0, step=0.1)\n",
    "    colsample_bylevel = trial.suggest_float('colsample_bylevel', 0.7, 1.0, step=0.1)\n",
    "    min_child_samples = trial.suggest_int('min_child_samples', 5, 50)\n",
    "    \n",
    "    # Try dynamic adjustments for tree architecture\n",
    "    grow_policy = trial.suggest_categorical('grow_policy', ['Depthwise', 'Lossguide'])\n",
    "    \n",
    "    # Create CatBoost model with discretized hyperparameters\n",
    "    model = CatBoostRegressor(\n",
    "        learning_rate=learning_rate,\n",
    "        depth=depth,\n",
    "        iterations=iterations,\n",
    "        l2_leaf_reg=l2_leaf_reg,\n",
    "        subsample=subsample,\n",
    "        colsample_bylevel=colsample_bylevel,\n",
    "        grow_policy=grow_policy,\n",
    "        min_data_in_leaf=min_child_samples,  # Works well with Lossguide grow policy\n",
    "        random_state=42,\n",
    "        silent=True,\n",
    "        thread_count=36\n",
    "    )\n",
    "    \n",
    "    # Cross-validation using KFold\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error', n_jobs=18)\n",
    "    mean_score = -scores.mean()\n",
    "    \n",
    "    return mean_score\n",
    "\n",
    "# Create an Optuna study with smarter sampling and pruning\n",
    "def optimize_model():\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize', \n",
    "        sampler=optuna.samplers.TPESampler(seed=42),  # TPE for structured sampling\n",
    "        pruner=optuna.pruners.HyperbandPruner(min_resource=100, max_resource=1000)  # Advanced pruning\n",
    "    )\n",
    "    \n",
    "    # Optimize the study\n",
    "    study.optimize(objective, n_trials=500, timeout=12000)  # Limiting number of trials\n",
    "    \n",
    "    # Get best result and display the results\n",
    "    print(f'Best MSE: {study.best_value}')\n",
    "    print(f'Best hyperparameters: {study.best_params}')\n",
    "    \n",
    "    # Visualizations (useful for deep dives)\n",
    "    optuna.visualization.plot_optimization_history(study).show()\n",
    "    optuna.visualization.plot_param_importances(study).show()\n",
    "\n",
    "    return study\n",
    "\n",
    "# Run the optimization\n",
    "best_study = optimize_model()\n",
    "\n",
    "# Train the model using best parameters\n",
    "best_params = best_study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee616a5-4761-45b2-9223-2cc94ede85cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a CatBoost Regressor\n",
    "catboost_model = CatBoostRegressor(learning_rate=0.1, depth=6, iterations=1000, random_state=42, silent=True, thread_count=36)\n",
    "# catboost_model= RandomForestRegressor(random_state=42)\n",
    "# Fit the model on the training data\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_catboost = catboost_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) and R-squared (R2) score on the test data\n",
    "mse_catboost = mean_squared_error(y_test, y_pred_catboost)\n",
    "r2_catboost = r2_score(y_test, y_pred_catboost)\n",
    "\n",
    "print(\"CatBoost Mean Squared Error:\", mse_catboost)\n",
    "print(\"CatBoost R-squared Score:\", r2_catboost)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# Perform cross-validation\n",
    "cv_scores_catboost = cross_val_score(catboost_model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error',n_jobs=18)\n",
    "\n",
    "# Calculate the average cross-validation score\n",
    "average_cv_score_catboost = cv_scores_catboost.mean()\n",
    "\n",
    "print(\"CatBoost Cross-Validation Scores:\", cv_scores_catboost)\n",
    "print(\"CatBoost Average Cross-Validation Score:\", -average_cv_score_catboost)\n",
    "\n",
    "# # Extract feature importances\n",
    "feature_importances_cb = catboost_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58221bdb-22b2-417a-b5aa-5160a4104198",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(catboost_model)\n",
    "shap_values1 = explainer.shap_values(X_test, check_additivity=False)\n",
    "shap_values_exp = shap.Explanation(\n",
    "    values=shap_values1,\n",
    "    base_values=np.repeat(explainer.expected_value, len(X_test)),\n",
    "    data=X_test.values,\n",
    "    feature_names=X_test.columns\n",
    ")\n",
    "shap.summary_plot(shap_values1, X_test, show=False)\n",
    "# Create the SHAP summary plot with enhancements\n",
    "def enhanced_shap_summary_plot(shap_values, features, feature_names=None, plot_type=\"dot\", title=None):\n",
    "    plt.figure(figsize=(12, 8))  # Adjust the figure size for better readability\n",
    "\n",
    "    # Generate the SHAP summary plot\n",
    "    shap.summary_plot(shap_values, features, feature_names=feature_names, plot_type=plot_type, show=False)\n",
    "\n",
    "    # Customize the title and labels\n",
    "    if title:\n",
    "        plt.title(title, fontsize=18, fontweight='bold', pad=20)\n",
    "    plt.xlabel('SHAP Value (Impact on Model Output)', fontsize=16, fontweight='bold')\n",
    "    # plt.ylabel('Features', fontsize=14, fontweight='bold')\n",
    "\n",
    "    \n",
    " # Add a bold black line at x=0\n",
    "    plt.axvline(x=0, color='black', linewidth=1)  # Central line at x=0\n",
    "    # Customize the tick labels for better readability\n",
    "    plt.xticks(fontsize=16, fontweight='bold')\n",
    "    plt.yticks(fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Add a grid to the plot for better visual structure\n",
    "    # plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Annotate the plot with additional insights\n",
    "    # Example: Highlight the top feature with the highest impact\n",
    "    # if feature_names is not None and len(feature_names) > 0:\n",
    "    #     top_feature = feature_names[0]\n",
    "    #     plt.annotate(f'Top Feature: {top_feature}', xy=(0.05, 0.95), xycoords='axes fraction', fontsize=14, color='darkblue', fontweight='bold')\n",
    "\n",
    "    # Customize the color bar (if relevant)\n",
    "    cbar = plt.gcf().axes[-1]\n",
    "    cbar.tick_params(labelsize=16)\n",
    "    cbar.set_ylabel('Feature Value', fontsize=16, fontweight='bold')\n",
    "\n",
    "    \n",
    "    # Find the \"Low\" and \"High\" labels and make them bold\n",
    "    for text in cbar.get_yticklabels():\n",
    "        text.set_fontweight('bold')\n",
    "        \n",
    "    # # Adjust the labels directly for \"Low\" and \"High\"\n",
    "    # for label in cbar.get_children():\n",
    "    #     if isinstance(label, mpl.text.Text):\n",
    "    #         label.set_fontsize(14)\n",
    "    #         label.set_fontweight('bold')\n",
    "\n",
    "    # Save the enhanced plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Bg_shap_summary_plot.pdf', format='pdf', dpi=2000, bbox_inches='tight', transparent=True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming shap_values1 and X_test are already defined\n",
    "enhanced_shap_summary_plot(shap_values1, X_test, feature_names=X_test.columns.tolist(), title='Bandgap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c9492e-49f0-49c2-9f14-67a7f2fc5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming feature_importances_rf and X.columns are already defined\n",
    "feature_names = X.columns\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_indices = np.argsort(feature_importances_cb)\n",
    "sorted_feature_names = [feature_names[i] for i in sorted_indices]\n",
    "sorted_importances = [feature_importances_cb[i] for i in sorted_indices]\n",
    "\n",
    "# Select the top 10 features\n",
    "top_10_feature_names = sorted_feature_names[-20:]\n",
    "top_10_importances = sorted_importances[-20:]\n",
    "\n",
    "# Create a figure and axis with adjusted margins\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplots_adjust(left=0.3, right=0.9)  # Adjusted margins: more space on the left and right\n",
    "\n",
    "# Use a color map for color gradients\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(top_10_feature_names)))\n",
    "\n",
    "# Create the horizontal bar plot\n",
    "plt.barh(top_10_feature_names, top_10_importances, color=colors)\n",
    "\n",
    "# Display importance values on the bars\n",
    "for index, value in enumerate(top_10_importances):\n",
    "    plt.text(value * 0.96, index, f\"{value:.4f}\", va='center', ha='left', color='black', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Customize plot labels and aesthetics\n",
    "# plt.xlabel(\"Feature Importance\", fontsize=16, fontweight='bold')\n",
    "# plt.ylabel(\"Feature\", fontsize=16, fontweight='bold')\n",
    "plt.title(\"Bandgap\", fontsize=18, fontweight='bold', pad=15)\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(rotation=0, fontsize=12, fontweight='bold')  # No rotation for feature names\n",
    "\n",
    "# Add grid lines, but make them subtle\n",
    "# plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust the layout to ensure everything fits well\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig('Bandgap_feature_importances.pdf', format='pdf', dpi=2000, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf2cc84-5e20-4d54-939a-ca00214a7b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, X, y, cv, train_sizes=np.linspace(.1, 1.0, 10)):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, train_sizes=train_sizes, scoring='r2', n_jobs=-1)\n",
    "\n",
    "    # Calculate the mean for training and test scores\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    # Set up the figure\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.title(\"Bandgap\", fontsize=20, fontweight='bold')\n",
    "    plt.xlabel(\"Training Examples\", fontsize=16, fontweight='bold')\n",
    "    plt.ylabel(\"R² Score\", fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Plot the learning curve\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\", linewidth=2, markersize=8)\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Test score\", linewidth=2, markersize=8)\n",
    "\n",
    "    # Set tick parameters for better readability\n",
    "    plt.xticks(fontsize=12, fontweight='bold')\n",
    "    plt.yticks(fontsize=12, fontweight='bold')\n",
    "\n",
    "    # Add grid for easier interpretation\n",
    "    # plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Highlight the initial learning phase\n",
    "    plt.axvline(train_sizes[0], color='b', linestyle='--', linewidth=2, label=\"Initial Training Size\")\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend(loc=\"best\", fontsize=14, frameon=True, fancybox=True)\n",
    "\n",
    "    # Save the plot as a high-resolution image\n",
    "    plt.savefig('Bandgap_learning_curve.pdf', format='pdf', dpi=2000, bbox_inches='tight')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Plot the learning curve for your Random Forest Regressor\n",
    "plot_learning_curve(catboost_model, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967eb9d7-cb49-45c9-86d4-564798b20854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parity(y_true, y_pred, title='Parity Plot'):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    # sns.scatterplot(x=y_true, y=y_pred, s=100, alpha=0.6, color='blue', edgecolor='k', linewidth=1)\n",
    "\n",
    "    plt.scatter(y_test, y_test, color='blue', marker='o', label='Actual Values')\n",
    "\n",
    "    # Scatter plot for predicted values (red color, 'x' marker)\n",
    "    plt.scatter(y_test, y_pred_catboost, color='red', marker='x', label='Predicted Values')\n",
    "    \n",
    "    # Plot the ideal line (y = x)\n",
    "    # plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], '--', color='red', linewidth=2)\n",
    "    \n",
    "    plt.title(title, fontsize=20, fontweight='bold')\n",
    "    plt.xlabel('Actual Value', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('Predicted Value', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.xticks(fontsize=12, fontweight='bold')\n",
    "    plt.yticks(fontsize=12, fontweight='bold')\n",
    "    plt.legend(loc=\"best\",fontsize=14)  # Show legend indicating which points correspond to actual and predicted values\n",
    "    # plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.savefig('Bandgap_parity_plot.pdf', format='pdf', dpi=2000, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_parity(y_test, y_pred_catboost, title='Bandap')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
